{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUk8zW7qZatHIG21treuSm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sazaqa0901/ML_test/blob/main/tester1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform ë°ì´í„° ì¦ê°•"
      ],
      "metadata": {
        "id": "ktpY3flnolZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GL09-Ub-9e6A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "j5dNavOw9idv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507dea18-5443-4878-c15d-585dc8817ecd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file_name = '/content/drive/MyDrive/Dataset.zip'\n",
        "extraction_dir = '/content/dataset'\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extraction_dir)"
      ],
      "metadata": {
        "id": "j92-6h_N9i56"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- ì„¤ì • ---\n",
        "DATA_ROOT = '/content/dataset/Dataset/Train' # ë°ì´í„°ê°€ ìˆëŠ” ë£¨íŠ¸ í´ë”\n",
        "SAMPLE_SIZE_FOR_SIZE_CHECK = 1000 # í¬ê¸° ë¶„ì„ì„ ìœ„í•´ ëª‡ ì¥ì„ ìƒ˜í”Œë§í• ì§€\n",
        "\n",
        "# ì´ë¯¸ì§€ë¡œ ê°„ì£¼í•  í™•ì¥ìë“¤\n",
        "IMAGE_EXTS = {'.jpg'}\n",
        "\n",
        "def analyze_folder(folder_path):\n",
        "    \"\"\"\n",
        "    í´ë” ë‚´ì˜ íŒŒì¼ ê°œìˆ˜, í™•ì¥ì ë¶„í¬, ê·¸ë¦¬ê³  ì´ë¯¸ì§€ í¬ê¸° í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        return None\n",
        "\n",
        "    total_files = 0\n",
        "    extension_counts = Counter()\n",
        "    all_image_paths = []\n",
        "\n",
        "    # 1. íŒŒì¼ ìŠ¤ìº” (os.walkë¡œ êµ¬ì„êµ¬ì„ ì°¾ê¸°)\n",
        "    print(f\"   ğŸ“‚ ìŠ¤ìº” ì¤‘... '{os.path.basename(folder_path)}'\")\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            total_files += 1\n",
        "            ext = os.path.splitext(file)[1].lower()\n",
        "            extension_counts[ext] += 1\n",
        "\n",
        "            if ext in IMAGE_EXTS:\n",
        "                all_image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    # 2. ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„ (ìƒ˜í”Œë§)\n",
        "    width_stats = {'min': 0, 'max': 0, 'mean': 0}\n",
        "    height_stats = {'min': 0, 'max': 0, 'mean': 0}\n",
        "\n",
        "    if all_image_paths:\n",
        "        # ìƒ˜í”Œë§ (ì „ì²´ ê°œìˆ˜ê°€ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ì ìœ¼ë©´ ì „ì²´ ì‚¬ìš©)\n",
        "        if len(all_image_paths) > SAMPLE_SIZE_FOR_SIZE_CHECK:\n",
        "            sampled_paths = random.sample(all_image_paths, SAMPLE_SIZE_FOR_SIZE_CHECK)\n",
        "        else:\n",
        "            sampled_paths = all_image_paths\n",
        "\n",
        "        widths = []\n",
        "        heights = []\n",
        "\n",
        "        for img_path in tqdm(sampled_paths, desc=f\"   ğŸ“ í¬ê¸° ì¸¡ì • ì¤‘ ({len(sampled_paths)}ì¥)\", leave=False):\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    w, h = img.size\n",
        "                    widths.append(w)\n",
        "                    heights.append(h)\n",
        "            except Exception:\n",
        "                pass # ê¹¨ì§„ ì´ë¯¸ì§€ëŠ” ë¬´ì‹œ\n",
        "\n",
        "        if widths:\n",
        "            widths = np.array(widths)\n",
        "            heights = np.array(heights)\n",
        "            width_stats = {'min': widths.min(), 'max': widths.max(), 'mean': widths.mean()}\n",
        "            height_stats = {'min': heights.min(), 'max': heights.max(), 'mean': heights.mean()}\n",
        "\n",
        "    return {\n",
        "        'total': total_files,\n",
        "        'exts': extension_counts,\n",
        "        'w_stats': width_stats,\n",
        "        'h_stats': height_stats,\n",
        "        'img_count': len(all_image_paths) # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    print(f\"=== ë°ì´í„°ì…‹ ì •ë°€ ë¶„ì„ (ê°œìˆ˜ + í¬ê¸°) ===\")\n",
        "    print(f\"ëŒ€ìƒ ê²½ë¡œ: {os.path.abspath(DATA_ROOT)}\\n\")\n",
        "\n",
        "    if not os.path.exists(DATA_ROOT):\n",
        "        print(f\"âŒ ì˜¤ë¥˜: '{DATA_ROOT}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        subfolders = [f for f in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, f))]\n",
        "        subfolders.sort()\n",
        "    except Exception as e:\n",
        "        print(f\"ì˜¤ë¥˜: {e}\")\n",
        "        return\n",
        "\n",
        "    if not subfolders:\n",
        "        print(\"âŒ í•˜ìœ„ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    print(f\"{'í´ë”ëª…':<12} | {'íŒŒì¼ ìˆ˜':<9} | {'ì´ë¯¸ì§€ ìˆ˜':<9} | {'í‰ê·  í¬ê¸° (WxH)':<18} | {'í™•ì¥ì ë¶„í¬'}\")\n",
        "    print(\"-\" * 95)\n",
        "\n",
        "    total_images_sum = 0\n",
        "\n",
        "    for folder in subfolders:\n",
        "        folder_path = os.path.join(DATA_ROOT, folder)\n",
        "        result = analyze_folder(folder_path)\n",
        "\n",
        "        if result is None:\n",
        "            print(f\"{folder:<12} | {'ê²½ë¡œ ì—†ìŒ':<9} |\")\n",
        "            continue\n",
        "\n",
        "        # ê²°ê³¼ í¬ë§¤íŒ…\n",
        "        count_str = f\"{result['total']:,}\"\n",
        "        img_count_str = f\"{result['img_count']:,}\"\n",
        "\n",
        "        w_mean = result['w_stats']['mean']\n",
        "        h_mean = result['h_stats']['mean']\n",
        "        size_str = f\"{w_mean:.0f}x{h_mean:.0f}\" if w_mean > 0 else \"N/A\"\n",
        "\n",
        "        # ì£¼ìš” í™•ì¥ìë§Œ í‘œì‹œ (ìƒìœ„ 3ê°œ)\n",
        "        top_exts = result['exts'].most_common(3)\n",
        "        ext_str = \", \".join([f\"{k} {v}\" for k, v in top_exts])\n",
        "\n",
        "        print(f\"{folder:<12} | {count_str:<9} | {img_count_str:<9} | {size_str:<18} | {ext_str}\")\n",
        "\n",
        "        total_images_sum += result['img_count']\n",
        "\n",
        "    print(\"-\" * 95)\n",
        "    print(f\"ì´ ì´ë¯¸ì§€ íŒŒì¼ í•©ê³„: {total_images_sum:,} ì¥\")\n",
        "    print(\"â€» í‰ê·  í¬ê¸°ëŠ” í´ë”ë³„ ìµœëŒ€ 1,000ì¥ ìƒ˜í”Œë§ ê¸°ì¤€ì…ë‹ˆë‹¤.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZKqnshetqcQ",
        "outputId": "b0df3b7a-7257-4583-8174-2ec539a56c38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ë°ì´í„°ì…‹ ì •ë°€ ë¶„ì„ (ê°œìˆ˜ + í¬ê¸°) ===\n",
            "ëŒ€ìƒ ê²½ë¡œ: /content/dataset/Dataset/Train\n",
            "\n",
            "í´ë”ëª…          | íŒŒì¼ ìˆ˜      | ì´ë¯¸ì§€ ìˆ˜     | í‰ê·  í¬ê¸° (WxH)        | í™•ì¥ì ë¶„í¬\n",
            "-----------------------------------------------------------------------------------------------\n",
            "   ğŸ“‚ ìŠ¤ìº” ì¤‘... 'Fake'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake         | 70,001    | 70,001    | 256x256            | .jpg 70001\n",
            "   ğŸ“‚ ìŠ¤ìº” ì¤‘... 'Real'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real         | 70,001    | 70,001    | 256x256            | .jpg 70001\n",
            "-----------------------------------------------------------------------------------------------\n",
            "ì´ ì´ë¯¸ì§€ íŒŒì¼ í•©ê³„: 140,002 ì¥\n",
            "â€» í‰ê·  í¬ê¸°ëŠ” í´ë”ë³„ ìµœëŒ€ 1,000ì¥ ìƒ˜í”Œë§ ê¸°ì¤€ì…ë‹ˆë‹¤.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- ì„¤ì • ---\n",
        "SOURCE_DATA_DIR = '/content/dataset/Dataset/Train'\n",
        "DEST_DIR = './deepfake_60k_224px' # ìƒˆë¡œ ë§Œë“¤ì–´ì§ˆ í´ë” ì´ë¦„\n",
        "TARGET_SIZE = 224 # ëª©í‘œ í¬ê¸°\n",
        "TARGET_REAL_SAMPLES = 20000 # Real í´ë”ë‹¹ ìƒ˜í”Œë§ ê°œìˆ˜\n",
        "TARGET_FAKE_SAMPLES = 20000 # Fake í´ë”ë‹¹ ìƒ˜í”Œë§ ê°œìˆ˜\n",
        "\n",
        "# ì´ë¯¸ì§€ í™•ì¥ì ì •ì˜\n",
        "VALID_EXTENSIONS = ('.jpg',)\n",
        "\n",
        "def resize_with_pad(img, target_size):\n",
        "    \"\"\"\n",
        "    ì´ë¯¸ì§€ë¥¼ target_size x target_size ìº”ë²„ìŠ¤ ì¤‘ì•™ì—\n",
        "    ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì§•í•˜ì—¬ ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤ (ê²€ì€ìƒ‰ íŒ¨ë”©).\n",
        "    \"\"\"\n",
        "    # RGBA(íˆ¬ëª…)ì¸ ê²½ìš° RGBë¡œ ë³€í™˜ (ì €ì¥ ì‹œ ì˜¤ë¥˜ ë°©ì§€)\n",
        "    if img.mode == 'RGBA':\n",
        "        img = img.convert('RGB')\n",
        "\n",
        "    w, h = img.size\n",
        "\n",
        "    # ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ ê³„ì‚°\n",
        "    scale = target_size / max(w, h)\n",
        "    new_w = int(w * scale)\n",
        "    new_h = int(h * scale)\n",
        "\n",
        "    # ë¦¬ì‚¬ì´ì§• (LANCZOS: ê³ í’ˆì§ˆ)\n",
        "    img_resized = img.resize((new_w, new_h), Image.LANCZOS)\n",
        "\n",
        "    # ê²€ì€ìƒ‰ ë°°ê²½ ìƒì„±\n",
        "    new_img = Image.new(\"RGB\", (target_size, target_size), (0, 0, 0))\n",
        "\n",
        "    # ì¤‘ì•™ ì¢Œí‘œ ê³„ì‚°\n",
        "    paste_x = (target_size - new_w) // 2\n",
        "    paste_y = (target_size - new_h) // 2\n",
        "\n",
        "    # ë¶™ì—¬ë„£ê¸°\n",
        "    new_img.paste(img_resized, (paste_x, paste_y))\n",
        "\n",
        "    return new_img\n",
        "\n",
        "def find_all_images(root_dir):\n",
        "    \"\"\"os.walkë¡œ ëª¨ë“  í•˜ìœ„ ì´ë¯¸ì§€ ì°¾ê¸°\"\"\"\n",
        "    image_list = []\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(VALID_EXTENSIONS):\n",
        "                image_list.append(os.path.join(root, file))\n",
        "    return image_list\n",
        "\n",
        "def main():\n",
        "    print(f\"=== ë°ì´í„° ì „ì²˜ë¦¬(224px) ë° ì €ì¥ ì‹œì‘ ===\")\n",
        "\n",
        "    # ëª©ì ì§€ í´ë” ì´ˆê¸°í™”\n",
        "    if os.path.exists(DEST_DIR):\n",
        "        print(f\"ê¸°ì¡´ '{DEST_DIR}' í´ë” ì‚­ì œ ì¤‘...\")\n",
        "        shutil.rmtree(DEST_DIR)\n",
        "\n",
        "    os.makedirs(os.path.join(DEST_DIR, 'face_real'))\n",
        "    os.makedirs(os.path.join(DEST_DIR, 'face_fake'))\n",
        "\n",
        "    # --- 1. ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ---\n",
        "    print(\"\\n[1ë‹¨ê³„] ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ë° ìƒ˜í”Œë§...\")\n",
        "\n",
        "    # (1) Real\n",
        "    wiki_path = os.path.join(SOURCE_DATA_DIR, 'Real')\n",
        "    real_paths = find_all_images(wiki_path)\n",
        "    print(f\"  - Real: {len(real_paths)}ì¥ ë°œê²¬\")\n",
        "\n",
        "    if len(real_paths) > TARGET_REAL_SAMPLES:\n",
        "        real_paths = np.random.choice(real_paths, TARGET_REAL_SAMPLES, replace=False)\n",
        "    else:\n",
        "        real_paths = real_paths\n",
        "    real_paths = real_paths.tolist() # numpy arrayë¥¼ listë¡œ ë³€í™˜\n",
        "    print(f\"  - Real ìƒ˜í”Œë§: {len(real_paths)}ì¥ ì„ íƒ\")\n",
        "\n",
        "    # (2) Fake\n",
        "    fake_data_path = os.path.join(SOURCE_DATA_DIR, 'Fake')\n",
        "    fake_paths = find_all_images(fake_data_path)\n",
        "    print(f\"  - Fake: {len(fake_paths)}ì¥ ë°œê²¬\")\n",
        "\n",
        "    if len(fake_paths) > TARGET_FAKE_SAMPLES:\n",
        "        fake_paths = np.random.choice(fake_paths, TARGET_FAKE_SAMPLES, replace=False)\n",
        "    else:\n",
        "        fake_paths = fake_paths\n",
        "    fake_paths = fake_paths.tolist() # numpy arrayë¥¼ listë¡œ ë³€í™˜\n",
        "    print(f\"  - Fake ìƒ˜í”Œë§: {len(fake_paths)}ì¥ ì„ íƒ\")\n",
        "\n",
        "    \"\"\"\n",
        "    fake_categories = ['inpainting', 'insight', 'text2img']\n",
        "    fake_copy_list = [] # (ì›ë³¸ê²½ë¡œ, ì €ì¥ë íŒŒì¼ëª…)\n",
        "\n",
        "    for cat in fake_categories:\n",
        "        cat_path = os.path.join(SOURCE_DATA_DIR, cat)\n",
        "        cat_images = find_all_images(cat_path)\n",
        "        print(f\"  - Fake ({cat}): {len(cat_images)}ì¥ ë°œê²¬\")\n",
        "\n",
        "        # ìƒ˜í”Œë§\n",
        "        if len(cat_images) >= TARGET_FAKE_PER_FOLDER:\n",
        "            sampled = np.random.choice(cat_images, TARGET_FAKE_PER_FOLDER, replace=False).tolist()\n",
        "        else:\n",
        "            print(f\"    âš ï¸ {cat}: 1ë§Œì¥ ë¶€ì¡± -> ì „ì²´ ì‚¬ìš©\")\n",
        "            sampled = cat_images\n",
        "\n",
        "        for src_path in sampled:\n",
        "            filename = os.path.basename(src_path)\n",
        "            # íŒŒì¼ëª… ì¶©ëŒ ë°©ì§€\n",
        "            new_filename = f\"{cat}_{filename}\"\n",
        "            # í™•ì¥ìë¥¼ .jpgë¡œ í†µì¼ (ì„ íƒì‚¬í•­)\n",
        "            name_only = os.path.splitext(new_filename)[0]\n",
        "            new_filename = f\"{name_only}.jpg\"\n",
        "\n",
        "            fake_copy_list.append((src_path, new_filename))\"\"\"\n",
        "\n",
        "    total_files = len(real_paths) + len(fake_paths)\n",
        "    print(f\"\\n  => ì´ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {total_files}ì¥\")\n",
        "\n",
        "    # --- 2. ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì €ì¥ ---\n",
        "    print(\"\\n[2ë‹¨ê³„] ë¦¬ì‚¬ì´ì§•(224x224) ë° ì €ì¥ ì‹œì‘...\")\n",
        "\n",
        "    # (1) Real ì²˜ë¦¬\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for src_path in tqdm(real_paths, desc=\"Processing Real\"):\n",
        "        try:\n",
        "            with Image.open(src_path) as img:\n",
        "                # ë¦¬ì‚¬ì´ì¦ˆ + íŒ¨ë”© í•¨ìˆ˜ í˜¸ì¶œ\n",
        "                processed_img = resize_with_pad(img, TARGET_SIZE)\n",
        "\n",
        "                # ì €ì¥ ê²½ë¡œ\n",
        "                filename = os.path.basename(src_path)\n",
        "                name_only = os.path.splitext(filename)[0]\n",
        "                save_name = f\"{name_only}.jpg\" # jpgë¡œ í†µì¼\n",
        "                dst_path = os.path.join(DEST_DIR, 'face_real', save_name)\n",
        "\n",
        "                # ì €ì¥ (ì••ì¶•ë¥  90 ì •ë„ë©´ í™”ì§ˆ ì¢‹ìŒ)\n",
        "                processed_img.save(dst_path, quality=95)\n",
        "                success_count += 1\n",
        "        except Exception as e:\n",
        "            # print(f\"Error processing {src_path}: {e}\")\n",
        "            error_count += 1\n",
        "\n",
        "    # (2) Fake ì²˜ë¦¬\n",
        "    for src_path in tqdm(fake_paths, desc=\"Processing Fake\"):\n",
        "        try:\n",
        "            with Image.open(src_path) as img:\n",
        "                # ì´ë¯¸ 512x512ë¼ë„ 224x224ë¡œ ì¤„ì„\n",
        "                processed_img = resize_with_pad(img, TARGET_SIZE)\n",
        "\n",
        "                filename = os.path.basename(src_path)\n",
        "                name_only = os.path.splitext(filename)[0]\n",
        "                save_name = f\"{name_only}.jpg\" # jpgë¡œ í†µì¼\n",
        "                dst_path = os.path.join(DEST_DIR, 'face_fake', save_name)\n",
        "\n",
        "                processed_img.save(dst_path, quality=95)\n",
        "                success_count += 1\n",
        "        except Exception as e:\n",
        "            error_count += 1\n",
        "\n",
        "    print(f\"\\n=== ì‘ì—… ì™„ë£Œ! ===\")\n",
        "    print(f\"ì„±ê³µ: {success_count}ì¥, ì‹¤íŒ¨: {error_count}ì¥\")\n",
        "    print(f\"ì €ì¥ ìœ„ì¹˜: {os.path.abspath(DEST_DIR)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY7NFqjJufX-",
        "outputId": "ec8b598c-aec3-47a0-c69b-478f1996dac6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ë°ì´í„° ì „ì²˜ë¦¬(224px) ë° ì €ì¥ ì‹œì‘ ===\n",
            "\n",
            "[1ë‹¨ê³„] ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ë° ìƒ˜í”Œë§...\n",
            "  - Real: 70001ì¥ ë°œê²¬\n",
            "  - Real ìƒ˜í”Œë§: 20000ì¥ ì„ íƒ\n",
            "  - Fake: 70001ì¥ ë°œê²¬\n",
            "  - Fake ìƒ˜í”Œë§: 20000ì¥ ì„ íƒ\n",
            "\n",
            "  => ì´ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: 40000ì¥\n",
            "\n",
            "[2ë‹¨ê³„] ë¦¬ì‚¬ì´ì§•(224x224) ë° ì €ì¥ ì‹œì‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Real: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [01:33<00:00, 212.78it/s]\n",
            "Processing Fake: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [01:18<00:00, 254.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ì‘ì—… ì™„ë£Œ! ===\n",
            "ì„±ê³µ: 40000ì¥, ì‹¤íŒ¨: 0ì¥\n",
            "ì €ì¥ ìœ„ì¹˜: /content/deepfake_60k_224px\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "NUM_SAMPLES = 40000\n",
        "LEARNING_RATE = 5e-5\n",
        "PATIENCE = 5"
      ],
      "metadata": {
        "id": "vXSR9lRr-BUV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResizeWithPad:\n",
        "    def __init__(self, target_size):\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        w, h = img.size\n",
        "\n",
        "        scale = self.target_size / max(w, h)\n",
        "\n",
        "        new_w = int(w * scale)\n",
        "        new_h = int(h * scale)\n",
        "\n",
        "        # ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ\n",
        "        resized_img = img.resize((new_w, new_h), Image.LANCZOS)\n",
        "\n",
        "        # ê²€ì€ìƒ‰ ìº”ë²„ìŠ¤ ìƒì„±\n",
        "        canvas = Image.new(\"RGB\", (self.target_size, self.target_size), (0, 0, 0))\n",
        "\n",
        "        # ìº”ë²„ìŠ¤ ì¤‘ì•™ì— ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ë°°ì¹˜\n",
        "        pad_x = (self.target_size - new_w) // 2\n",
        "        pad_y = (self.target_size - new_h) // 2\n",
        "\n",
        "        canvas.paste(resized_img, (pad_x, pad_y))\n",
        "\n",
        "        return canvas"
      ],
      "metadata": {
        "id": "I-6_tqLl9igO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.resizer = ResizeWithPad(224)\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ë””ìŠ¤í¬ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ì´ë¯¸ì§€ ë¡œë“œ (PIL)\n",
        "        img_path = self.paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = self.resizer(image)\n",
        "\n",
        "\n",
        "        return image\"\"\""
      ],
      "metadata": {
        "id": "qDuryj259ilG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "315167ff-c1d4-4141-d7f3-11a37b2628cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"class DeepfakeDataset(Dataset):\\n    def __init__(self, paths, labels, transform=None):\\n        self.resizer = ResizeWithPad(224)\\n        self.paths = paths\\n        self.labels = labels\\n        self.transform = transform\\n\\n    def __len__(self):\\n        return len(self.paths)\\n\\n    def __getitem__(self, idx):\\n        # ë””ìŠ¤í¬ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ì´ë¯¸ì§€ ë¡œë“œ (PIL)\\n        img_path = self.paths[idx]\\n        image = Image.open(img_path).convert('RGB')\\n        image = self.resizer(image)\\n\\n\\n        return image\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1c347645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "f838844b-fa29-408c-bc8f-976b5887ac9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to_tensor = transforms.PILToTensor() # uint8 ìœ ì§€ (0-255)\\n    resizer = ResizeWithPad(224)\\n\\n    tensor_list = []\\n    print(f\"Loading {len(paths)} images to RAM (uint8 mode)...\")\\n\\n    for path in tqdm(paths):\\n        try:\\n            img = Image.open(path).convert(\\'RGB\\')\\n            img = resizer(img)\\n            tensor = to_tensor(img) # (3, 224, 224) uint8 Tensor\\n            tensor_list.append(tensor)\\n        except Exception as e:\\n            print(f\"Error loading {path}: {e}\")\\n\\n\\n    return torch.stack(tensor_list) # (N, 3, 224, 224) uint8 Tensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\"\"\"IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "class RAMDataset(Dataset):\n",
        "    def __init__(self, images_uint8, labels, transform=Nonã„·):\n",
        "        self.images = images_uint8 # (N, C, H, W) uint8 tensor\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        # ì •ê·œí™”ìš© transform (ì—¬ê¸°ì„œ floatë³€í™˜ ë° ì •ê·œí™” ìˆ˜í–‰)\n",
        "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. uint8 í…ì„œ ê°€ì ¸ì˜¤ê¸°\n",
        "        img = self.images[idx]\n",
        "\n",
        "        # 2. float32ë¡œ ë³€í™˜ ë° 0~1 ìŠ¤ì¼€ì¼ë§ (ë§¤ìš° ë¹ ë¦„)\n",
        "        img = img.float() / 255.0\n",
        "\n",
        "        # 3. ì •ê·œí™” ì ìš©\n",
        "        img = self.normalize(img)\n",
        "\n",
        "        # 4. ì¶”ê°€ ì¦ê°•(Augmentation)ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ì ìš© ê°€ëŠ¥\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "def load_images_to_ram_uint8(paths):\"\"\"\n",
        "    #\"\"\"\n",
        "   # ì´ë¯¸ì§€ë¥¼ ì½ì–´ì„œ ë¦¬ì‚¬ì´ì¦ˆ í›„ uint8 í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "   # \"\"\"\n",
        "\"\"\"to_tensor = transforms.PILToTensor() # uint8 ìœ ì§€ (0-255)\n",
        "    resizer = ResizeWithPad(224)\n",
        "\n",
        "    tensor_list = []\n",
        "    print(f\"Loading {len(paths)} images to RAM (uint8 mode)...\")\n",
        "\n",
        "    for path in tqdm(paths):\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img = resizer(img)\n",
        "            tensor = to_tensor(img) # (3, 224, 224) uint8 Tensor\n",
        "            tensor_list.append(tensor)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {path}: {e}\")\n",
        "\n",
        "\n",
        "    return torch.stack(tensor_list) # (N, 3, 224, 224) uint8 Tensor\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None, target_size=224):\n",
        "        self.paths = paths\n",
        "        # ë¼ë²¨ì„ Tensorë¡œ ë³€í™˜í•˜ê³  ì°¨ì›ì„ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
        "        self.labels = [torch.tensor(l, dtype=torch.float32).unsqueeze(0) for l in labels]\n",
        "        self.transform = transform\n",
        "\n",
        "        # 1. í¬ê¸° ì¡°ì • ë° íŒ¨ë”© (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì²« ë‹¨ê³„)\n",
        "        self.resizer = ResizeWithPad(target_size)\n",
        "\n",
        "        # 2. ìµœì¢… í…ì„œ ë³€í™˜ ë° ì •ê·œí™” (ì¦ê°• í›„ ë§ˆì§€ë§‰ ë‹¨ê³„)\n",
        "        # ì´ì „ ì½”ë“œì˜ IMAGENET_MEAN/STDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "        self.final_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.paths[idx]\n",
        "\n",
        "        # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° RGB ë³€í™˜\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # 2. í¬ê¸° ì¡°ì • ë° íŒ¨ë”© ì ìš©\n",
        "        image = self.resizer(image)\n",
        "\n",
        "        # 3. ë°ì´í„° ì¦ê°• ì ìš© (PIL Image ìƒíƒœ)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # 4. í…ì„œ ë³€í™˜ ë° ì •ê·œí™”\n",
        "        image = self.final_transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "woXFBga77rql"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetLike(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(AlexNetLike, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Conv 1\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), # 224 -> 55\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # 55 -> 27\n",
        "\n",
        "            # Conv 2\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2), # 27 -> 27\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # 27 -> 13\n",
        "\n",
        "            # Conv 3\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1), # 13 -> 13\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Conv 4\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1), # 13 -> 13\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Conv 5\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1), # 13 -> 13\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # 13 -> 6\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, num_classes) # num_classes=1\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x); x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1); x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WFqz2Y8P9iuf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_name, device):\n",
        "\n",
        "    model = None\n",
        "    num_classes = 1 # ì´ì§„ ë¶„ë¥˜ (Real/Fake)\n",
        "\n",
        "    print(f\"Loading {model_name} architecture (FROM SCRATCH)...\")\n",
        "\n",
        "    if model_name.lower() == 'alexnet':\n",
        "        # ì§ì ‘ ì§  AlexNet (Conv 5, FC 3)\n",
        "        model =  AlexNetLike(num_classes=num_classes)\n",
        "\n",
        "    elif model_name.lower() == 'vgg16':\n",
        "        # VGG16\n",
        "        model = models.vgg16(weights=None, num_classes=num_classes)\n",
        "\n",
        "    elif model_name.lower() == 'googlenet':\n",
        "        # GoogLeNet\n",
        "        model = models.googlenet(weights=None, num_classes=num_classes, aux_logits=False)\n",
        "\n",
        "    elif model_name.lower() == 'resnet50':\n",
        "        # ResNet50\n",
        "        model = models.resnet50(weights=None, num_classes=num_classes)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model name: {model_name}. Choose from 'alexnet, 'vgg16', 'googlenet', 'resnet50'\")\n",
        "\n",
        "    return model.to(device)"
      ],
      "metadata": {
        "id": "opsguJzt9izn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1989208d"
      },
      "outputs": [],
      "source": [
        "# í•™ìŠµ\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience):\n",
        "    print(\"=== í•™ìŠµ ì‹œì‘ ===\")\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_weights = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # --- í›ˆë ¨ ---\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
        "\n",
        "        for images, labels in train_pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # ìˆœì „íŒŒ\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # ì—­ì „íŒŒ ë° ìµœì í™”\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # ì •í™•ë„ ê³„ì‚°\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (preds == labels).sum().item()\n",
        "\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = correct_train / total_train\n",
        "\n",
        "        # --- ê²€ì¦ ---\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
        "            for images, labels in val_pbar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "                preds = torch.sigmoid(outputs) > 0.5\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (preds == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc = correct_val / total_val\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - {elapsed_time:.0f}s - \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} - \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # --- Early Stopping ë° Best Model ì €ì¥ ---\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            print(f\"  Validation loss decreased ({best_val_loss:.4f} --> {epoch_val_loss:.4f}). Saving model...\")\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_weights, MODEL_SAVE_PATH)\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    print(\"=== í•™ìŠµ ì™„ë£Œ ===\")\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_test_loss = 0.0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    print(\"\\n=== í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€ ì‹œì‘ ===\")\n",
        "    with torch.no_grad():\n",
        "        test_pbar = tqdm(test_loader, desc=\"[Test]\", leave=False)\n",
        "        for images, labels in test_pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_test_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (preds == labels).sum().item()\n",
        "\n",
        "    test_loss = running_test_loss / len(test_loader.dataset)\n",
        "    test_acc = correct_test / total_test\n",
        "\n",
        "    print(f\"===== ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ =====\")\n",
        "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "9bPDf2Ww9i3m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë©”ì¸ ì‹¤í–‰\n",
        "start_time = time.time()\n",
        "\n",
        "# GPU ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# ë°ì´í„° ê²½ë¡œ ë° ë¼ë²¨ ìˆ˜ì§‘\n",
        "Fake_PATH = \"/content/dataset/Dataset/Train/Fake\"\n",
        "Real_PATH = \"/content/dataset/Dataset/Train/Real\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/ML/deepfake_baseline_model.pth\"\n",
        "print(\"ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ì¤‘...\")\n",
        "face_real_dir = os.path.join(Real_PATH)\n",
        "face_fake_dir = os.path.join(Fake_PATH)\n",
        "\n",
        "real_paths = glob.glob(os.path.join(face_real_dir, \"*.*\"))\n",
        "fake_paths = glob.glob(os.path.join(face_fake_dir, \"*.*\"))\n",
        "\n",
        "all_paths = real_paths + fake_paths\n",
        "all_labels = [0] * len(real_paths) + [1] * len(fake_paths)\n",
        "\n",
        "print(f\"ì´ {len(all_labels)}ê°œ ì´ë¯¸ì§€ ê²½ë¡œ ë°œê²¬.\")\n",
        "# ìƒ˜í”Œ ê°œìˆ˜ ì œí•œ\n",
        "NUM_SAMPLES = min(NUM_SAMPLES, len(all_paths))\n",
        "\n",
        "print(f\"{NUM_SAMPLES}ê°œ ìƒ˜í”Œì„ ìƒ˜í”Œë§...\")\n",
        "_, target_paths, _, target_labels = train_test_split(\n",
        "    all_paths, all_labels,\n",
        "    test_size= NUM_SAMPLES,\n",
        "    random_state=42,\n",
        "    stratify=all_labels\n",
        ")\n",
        "print(f\"ìƒ˜í”Œë§ ì™„ë£Œ: {NUM_SAMPLES}ê°œ ì´ë¯¸ì§€ ì„ íƒ.\")\n",
        "# 10000ê°œ ìƒ˜í”Œë§\n",
        "target_paths = all_paths\n",
        "target_labels = all_labels\n",
        "\n",
        "# ì „ì²´ë¥¼ ì“°ë”ë¼ë„ í•™ìŠµì„ ìœ„í•´ ì„ì–´ì¤ë‹ˆë‹¤.\n",
        "combined = list(zip(target_paths, target_labels))\n",
        "np.random.shuffle(combined)\n",
        "target_paths[:], target_labels[:] = zip(*combined)"
      ],
      "metadata": {
        "id": "TQdrB4Mh-BKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3e7981-abb4-45ab-c035-b948d7fea119"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ì¤‘...\n",
            "ì´ 140002ê°œ ì´ë¯¸ì§€ ê²½ë¡œ ë°œê²¬.\n",
            "40000ê°œ ìƒ˜í”Œì„ ìƒ˜í”Œë§...\n",
            "ìƒ˜í”Œë§ ì™„ë£Œ: 40000ê°œ ì´ë¯¸ì§€ ì„ íƒ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ë°ì´í„°ë¥¼ 7:2:1 ë¹„ìœ¨ë¡œ ë¶„í• í•©ë‹ˆë‹¤...\")\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    target_paths, target_labels, test_size=0.3, random_state=42, stratify=target_labels\n",
        ")\n",
        "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=(1/3), random_state=42, stratify=temp_labels\n",
        ")\n",
        "print(f\"ë¶„í•  ì™„ë£Œ: Train {len(train_paths)}ê°œ, Validation {len(val_paths)}ê°œ, Test {len(test_paths)}ê°œ\")"
      ],
      "metadata": {
        "id": "CdMnbU6J-BIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee92c39-6742-42fa-d475-13d392eedb2d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°ì´í„°ë¥¼ 7:2:1 ë¹„ìœ¨ë¡œ ë¶„í• í•©ë‹ˆë‹¤...\n",
            "ë¶„í•  ì™„ë£Œ: Train 98001ê°œ, Validation 28000ê°œ, Test 14001ê°œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ì „ì²˜ë¦¬ ë° RAMì— ë¡œë“œ\n",
        "print(\"ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ë¡œë“œ)...\")\n",
        "\"\"\"\n",
        "# ëª¨ë“  ë°ì´í„°ë¥¼ RAMìœ¼ë¡œ ë¡œë“œ\n",
        "X_train = load_images_to_ram_uint8(train_paths)\n",
        "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_val = load_images_to_ram_uint8(val_paths)\n",
        "y_val = torch.tensor(val_labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test = load_images_to_ram_uint8(test_paths)\n",
        "y_test = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "print(\"ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ë¡œë“œ ì™„ë£Œ.\")\n",
        "\n",
        "# RAM ê¸°ë°˜ì˜ TensorDatasetê³¼ DataLoader ìƒì„±\n",
        "train_dataset = RAMDataset(X_train, y_train, transform=None)\n",
        "val_dataset = RAMDataset(X_val, y_val, transform=None)\n",
        "test_dataset = RAMDataset(X_test, y_test, transform=None)\n",
        "\n",
        "# RAMì—ì„œ ì½ìœ¼ë¯€ë¡œ num_workers=0, pin_memory=False (ì´ë¯¸ RAMì— ìˆìŒ)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ.\")\"\"\"\n",
        "# ê¸°ì¡´ RAM ë¡œë”© ì½”ë“œ ëª¨ë‘ ì œê±° (X_train, X_val, X_test = load_images_to_ram_uint8(...) ë¶€ë¶„)\n",
        "# ëŒ€ì‹  ì•„ë˜ ì½”ë“œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
        "\n",
        "# --- 1. í›ˆë ¨ìš© ë°ì´í„° ì¦ê°• ì •ì˜ ---\n",
        "IMG_SIZE = 224 # ì´ì „ ì„¤ì • ì‚¬ìš©\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    # RandomResizedCropì€ ì´ë¯¸ ResizeWithPadë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì ì‹œ ì œì™¸í•˜ê±°ë‚˜\n",
        "    # transforms.ColorJitter(brightness=0.1, contrast=0.1) ë“± ì¶”ê°€ ê¶Œì¥\n",
        "])\n",
        "\n",
        "# --- 2. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„± ---\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„°ì…‹ (ì¦ê°• ì ìš©)\n",
        "train_dataset = DeepfakeDataset(train_paths, train_labels, transform=train_transform, target_size=IMG_SIZE)\n",
        "# ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ì¦ê°• ì—†ìŒ)\n",
        "val_dataset = DeepfakeDataset(val_paths, val_labels, transform=None, target_size=IMG_SIZE)\n",
        "test_dataset = DeepfakeDataset(test_paths, test_labels, transform=None, target_size=IMG_SIZE)\n",
        "\n",
        "print(\"ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ.\")\n",
        "\n",
        "# GPUë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, num_workersë¥¼ ëŠ˜ë ¤ CPUì˜ I/O ë³‘ëª© í˜„ìƒ í•´ì†Œ\n",
        "# ë³´í†µ 2~8 ì‚¬ì´ë¥¼ ì‚¬ìš©í•˜ë©°, Colab T4 GPU í™˜ê²½ì—ì„œëŠ” 4~8ì´ ì ì ˆí•©ë‹ˆë‹¤.\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ (num_workers={NUM_WORKERS}).\")"
      ],
      "metadata": {
        "id": "b2bGm-Z--BFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7363a3b7-d280-49d5-e145-53faa63200f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ë¡œë“œ)...\n",
            "ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ.\n",
            "ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ (num_workers=4).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# 8. ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
        "model = get_model('alexnet', device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 9. í•™ìŠµ ë° í‰ê°€\n",
        "model = train_model(model, train_loader, val_loader, criterion, optimizer, device, EPOCHS, PATIENCE)\n",
        "evaluate_model(model, test_loader,\n",
        "               criterion, device)\n",
        "\n",
        "print(f\"ì´ ì‹¤í–‰ ì‹œê°„: {(time.time() - start_time) / 60:.2f} ë¶„\")"
      ],
      "metadata": {
        "id": "aZnszUtP-BAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78c68e6-bed2-4c1e-de51-0de257f75b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading alexnet architecture (FROM SCRATCH)...\n",
            "=== í•™ìŠµ ì‹œì‘ ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/30 [Train]:   0%|          | 0/1532 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1/30 [Train]:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 564/1532 [1:30:06<2:43:43, 10.15s/it, loss=0.468]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_predictions(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    print(\"\\n=== í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ ìˆ˜ì§‘ ì‹œì‘ ===\")\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"[Prediction]\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.cpu().numpy() # ì‹¤ì œ ë¼ë²¨\n",
        "\n",
        "            outputs = model(images)\n",
        "            # 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ (0 ë˜ëŠ” 1)ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).int().squeeze(1).cpu().numpy() # ì˜ˆì¸¡ ë¼ë²¨\n",
        "\n",
        "            all_labels.extend(labels)\n",
        "            all_preds.extend(preds)\n",
        "\n",
        "    # ê²°ê³¼ë¥¼ NumPy ë°°ì—´ë¡œ ë°˜í™˜\n",
        "    return np.array(all_labels), np.array(all_preds)"
      ],
      "metadata": {
        "id": "jEHi-O5QOnEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (ë©”ì¸ ì‹¤í–‰ ì½”ë“œì— ì¶”ê°€)\n",
        "# ëª¨ë¸ ë¡œë”© ë° í…ŒìŠ¤íŠ¸ ë¡œë”ëŠ” ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "true_labels, predicted_labels = get_predictions(model, test_loader, device)"
      ],
      "metadata": {
        "id": "lqT8tsqpOpGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# í˜¼ë™ í–‰ë ¬ ê³„ì‚°\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬ (0: Real, 1: Fake)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=['Actual Real (0)', 'Actual Fake (1)'],\n",
        "                     columns=['Predicted Real (0)', 'Predicted Fake (1)'])\n",
        "\n",
        "print(\"\\n===== í˜¼ë™ í–‰ë ¬ (Confusion Matrix) =====\")\n",
        "print(cm_df)"
      ],
      "metadata": {
        "id": "zkvIOOIHOtaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}