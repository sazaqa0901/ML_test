{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a57e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec8ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë°ì´í„°ì…‹ ì •ë°€ ë¶„ì„ (ê°œìˆ˜ + í¬ê¸°) ===\n",
      "ëŒ€ìƒ ê²½ë¡œ: C:\\Users\\user\\Downloads\\Dataset\\Dataset\n",
      "\n",
      "í´ë”ëª…          | íŒŒì¼ ìˆ˜      | ì´ë¯¸ì§€ ìˆ˜     | í‰ê·  í¬ê¸° (WxH)        | í™•ì¥ì ë¶„í¬\n",
      "-----------------------------------------------------------------------------------------------\n",
      "   ğŸ“‚ ìŠ¤ìº” ì¤‘... 'Test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test         | 10,906    | 10,905    | 256x256            | .jpg 10905,  1\n",
      "   ğŸ“‚ ìŠ¤ìº” ì¤‘... 'Train'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train        | 140,002   | 140,002   | 256x256            | .jpg 140002\n",
      "   ğŸ“‚ ìŠ¤ìº” ì¤‘... 'Validation'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation   | 39,428    | 39,428    | 256x256            | .jpg 39428\n",
      "-----------------------------------------------------------------------------------------------\n",
      "ì´ ì´ë¯¸ì§€ íŒŒì¼ í•©ê³„: 190,335 ì¥\n",
      "â€» í‰ê·  í¬ê¸°ëŠ” í´ë”ë³„ ìµœëŒ€ 1,000ì¥ ìƒ˜í”Œë§ ê¸°ì¤€ì…ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "DATA_ROOT = 'C:/Users/user/Downloads/Dataset/Dataset' # ë°ì´í„°ê°€ ìˆëŠ” ë£¨íŠ¸ í´ë”\n",
    "SAMPLE_SIZE_FOR_SIZE_CHECK = 1000 # í¬ê¸° ë¶„ì„ì„ ìœ„í•´ ëª‡ ì¥ì„ ìƒ˜í”Œë§í• ì§€\n",
    "\n",
    "# ì´ë¯¸ì§€ë¡œ ê°„ì£¼í•  í™•ì¥ìë“¤\n",
    "IMAGE_EXTS = {'.jpg'}\n",
    "\n",
    "def analyze_folder(folder_path):\n",
    "    \"\"\"\n",
    "    í´ë” ë‚´ì˜ íŒŒì¼ ê°œìˆ˜, í™•ì¥ì ë¶„í¬, ê·¸ë¦¬ê³  ì´ë¯¸ì§€ í¬ê¸° í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        return None\n",
    "\n",
    "    total_files = 0\n",
    "    extension_counts = Counter()\n",
    "    all_image_paths = []\n",
    "\n",
    "    # 1. íŒŒì¼ ìŠ¤ìº” (os.walkë¡œ êµ¬ì„êµ¬ì„ ì°¾ê¸°)\n",
    "    print(f\"   ğŸ“‚ ìŠ¤ìº” ì¤‘... '{os.path.basename(folder_path)}'\")\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            total_files += 1\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            extension_counts[ext] += 1\n",
    "            \n",
    "            if ext in IMAGE_EXTS:\n",
    "                all_image_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    # 2. ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„ (ìƒ˜í”Œë§)\n",
    "    width_stats = {'min': 0, 'max': 0, 'mean': 0}\n",
    "    height_stats = {'min': 0, 'max': 0, 'mean': 0}\n",
    "    \n",
    "    if all_image_paths:\n",
    "        # ìƒ˜í”Œë§ (ì „ì²´ ê°œìˆ˜ê°€ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ì ìœ¼ë©´ ì „ì²´ ì‚¬ìš©)\n",
    "        if len(all_image_paths) > SAMPLE_SIZE_FOR_SIZE_CHECK:\n",
    "            sampled_paths = random.sample(all_image_paths, SAMPLE_SIZE_FOR_SIZE_CHECK)\n",
    "        else:\n",
    "            sampled_paths = all_image_paths\n",
    "            \n",
    "        widths = []\n",
    "        heights = []\n",
    "        \n",
    "        for img_path in tqdm(sampled_paths, desc=f\"   ğŸ“ í¬ê¸° ì¸¡ì • ì¤‘ ({len(sampled_paths)}ì¥)\", leave=False):\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    widths.append(w)\n",
    "                    heights.append(h)\n",
    "            except Exception:\n",
    "                pass # ê¹¨ì§„ ì´ë¯¸ì§€ëŠ” ë¬´ì‹œ\n",
    "        \n",
    "        if widths:\n",
    "            widths = np.array(widths)\n",
    "            heights = np.array(heights)\n",
    "            width_stats = {'min': widths.min(), 'max': widths.max(), 'mean': widths.mean()}\n",
    "            height_stats = {'min': heights.min(), 'max': heights.max(), 'mean': heights.mean()}\n",
    "\n",
    "    return {\n",
    "        'total': total_files,\n",
    "        'exts': extension_counts,\n",
    "        'w_stats': width_stats,\n",
    "        'h_stats': height_stats,\n",
    "        'img_count': len(all_image_paths) # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(f\"=== ë°ì´í„°ì…‹ ì •ë°€ ë¶„ì„ (ê°œìˆ˜ + í¬ê¸°) ===\")\n",
    "    print(f\"ëŒ€ìƒ ê²½ë¡œ: {os.path.abspath(DATA_ROOT)}\\n\")\n",
    "\n",
    "    if not os.path.exists(DATA_ROOT):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: '{DATA_ROOT}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        subfolders = [f for f in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, f))]\n",
    "        subfolders.sort()\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜: {e}\")\n",
    "        return\n",
    "\n",
    "    if not subfolders:\n",
    "        print(\"âŒ í•˜ìœ„ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{'í´ë”ëª…':<12} | {'íŒŒì¼ ìˆ˜':<9} | {'ì´ë¯¸ì§€ ìˆ˜':<9} | {'í‰ê·  í¬ê¸° (WxH)':<18} | {'í™•ì¥ì ë¶„í¬'}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    total_images_sum = 0\n",
    "\n",
    "    for folder in subfolders:\n",
    "        folder_path = os.path.join(DATA_ROOT, folder)\n",
    "        result = analyze_folder(folder_path)\n",
    "        \n",
    "        if result is None:\n",
    "            print(f\"{folder:<12} | {'ê²½ë¡œ ì—†ìŒ':<9} |\")\n",
    "            continue\n",
    "            \n",
    "        # ê²°ê³¼ í¬ë§¤íŒ…\n",
    "        count_str = f\"{result['total']:,}\"\n",
    "        img_count_str = f\"{result['img_count']:,}\"\n",
    "        \n",
    "        w_mean = result['w_stats']['mean']\n",
    "        h_mean = result['h_stats']['mean']\n",
    "        size_str = f\"{w_mean:.0f}x{h_mean:.0f}\" if w_mean > 0 else \"N/A\"\n",
    "        \n",
    "        # ì£¼ìš” í™•ì¥ìë§Œ í‘œì‹œ (ìƒìœ„ 3ê°œ)\n",
    "        top_exts = result['exts'].most_common(3)\n",
    "        ext_str = \", \".join([f\"{k} {v}\" for k, v in top_exts])\n",
    "        \n",
    "        print(f\"{folder:<12} | {count_str:<9} | {img_count_str:<9} | {size_str:<18} | {ext_str}\")\n",
    "        \n",
    "        total_images_sum += result['img_count']\n",
    "\n",
    "    print(\"-\" * 95)\n",
    "    print(f\"ì´ ì´ë¯¸ì§€ íŒŒì¼ í•©ê³„: {total_images_sum:,} ì¥\")\n",
    "    print(\"â€» í‰ê·  í¬ê¸°ëŠ” í´ë”ë³„ ìµœëŒ€ 1,000ì¥ ìƒ˜í”Œë§ ê¸°ì¤€ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31604d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "NUM_SAMPLES = 20000\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "671e6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeWithPad:\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "\n",
    "        scale = self.target_size / max(w, h)\n",
    "\n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "\n",
    "        # ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ\n",
    "        resized_img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "        # ê²€ì€ìƒ‰ ìº”ë²„ìŠ¤ ìƒì„±\n",
    "        canvas = Image.new(\"RGB\", (self.target_size, self.target_size), (0, 0, 0))\n",
    "\n",
    "        # ìº”ë²„ìŠ¤ ì¤‘ì•™ì— ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ë°°ì¹˜\n",
    "        pad_x = (self.target_size - new_w) // 2\n",
    "        pad_y = (self.target_size - new_h) // 2\n",
    "\n",
    "        canvas.paste(resized_img, (pad_x, pad_y))\n",
    "\n",
    "        return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8795f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "class RAMDataset(Dataset):\n",
    "    def __init__(self, images_uint8, labels, transform=None):\n",
    "        self.images = images_uint8 # (N, C, H, W) uint8 tensor\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        # ì •ê·œí™”ìš© transform (ì—¬ê¸°ì„œ floatë³€í™˜ ë° ì •ê·œí™” ìˆ˜í–‰)\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. uint8 í…ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "        img = self.images[idx]\n",
    "\n",
    "        # 2. float32ë¡œ ë³€í™˜ ë° 0~1 ìŠ¤ì¼€ì¼ë§ (ë§¤ìš° ë¹ ë¦„)\n",
    "        img = img.float() / 255.0\n",
    "\n",
    "        # 3. ì •ê·œí™” ì ìš©\n",
    "        img = self.normalize(img)\n",
    "\n",
    "        # 4. ì¶”ê°€ ì¦ê°•(Augmentation)ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ì ìš© ê°€ëŠ¥\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "def load_images_to_ram_uint8(paths):\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ë¥¼ ì½ì–´ì„œ ë¦¬ì‚¬ì´ì¦ˆ í›„ uint8 í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
    "    \"\"\"\n",
    "    to_tensor = transforms.PILToTensor() # uint8 ìœ ì§€ (0-255)\n",
    "    resizer = ResizeWithPad(224)\n",
    "\n",
    "    tensor_list = []\n",
    "    print(f\"Loading {len(paths)} images to RAM (uint8 mode)...\")\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img = resizer(img)\n",
    "            tensor = to_tensor(img) # (3, 224, 224) uint8 Tensor\n",
    "            tensor_list.append(tensor)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "\n",
    "    return torch.stack(tensor_list) # (N, 3, 224, 224) uint8 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abdb547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetLike(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(AlexNetLike, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv 1\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), # 224 -> 55\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 55 -> 27\n",
    "\n",
    "            # Conv 2\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2), # 27 -> 27\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 27 -> 13\n",
    "\n",
    "            # Conv 3\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), # 13 -> 13\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Conv 4\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), # 13 -> 13\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Conv 5\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1), # 13 -> 13\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 13 -> 6\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 6 * 6, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes) # num_classes=1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x); x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1); x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b68878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, device):\n",
    "\n",
    "    model = None\n",
    "    num_classes = 1 # ì´ì§„ ë¶„ë¥˜ (Real/Fake)\n",
    "\n",
    "    print(f\"Loading {model_name} architecture (FROM SCRATCH)...\")\n",
    "\n",
    "    if model_name.lower() == 'alexnet':\n",
    "        # ì§ì ‘ ì§  AlexNet (Conv 5, FC 3)\n",
    "        model =  AlexNetLike(num_classes=num_classes)\n",
    "\n",
    "    elif model_name.lower() == 'vgg16':\n",
    "        # VGG16\n",
    "        model = models.vgg16(weights=None, num_classes=num_classes)\n",
    "\n",
    "    elif model_name.lower() == 'googlenet':\n",
    "        # GoogLeNet\n",
    "        model = models.googlenet(weights=None, num_classes=num_classes, aux_logits=False)\n",
    "\n",
    "    elif model_name.lower() == 'resnet50':\n",
    "        # ResNet50\n",
    "        model = models.resnet50(weights=None, num_classes=num_classes)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}. Choose from 'alexnet, 'vgg16', 'googlenet', 'resnet50'\")\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16b9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience):\n",
    "    print(\"=== í•™ìŠµ ì‹œì‘ ===\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # --- í›ˆë ¨ ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "\n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # ìˆœì „íŒŒ\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # ì—­ì „íŒŒ ë° ìµœì í™”\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # ì •í™•ë„ ê³„ì‚°\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct_train / total_train\n",
    "\n",
    "        # --- ê²€ì¦ ---\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct_val / total_val\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - {elapsed_time:.0f}s - \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} - \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        # --- Early Stopping ë° Best Model ì €ì¥ ---\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            print(f\"  Validation loss decreased ({best_val_loss:.4f} --> {epoch_val_loss:.4f}). Saving model...\")\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_weights, MODEL_SAVE_PATH)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(\"=== í•™ìŠµ ì™„ë£Œ ===\")\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5821e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    print(\"\\n=== í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€ ì‹œì‘ ===\")\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc=\"[Test]\", leave=False)\n",
    "        for images, labels in test_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_test_loss += loss.item() * images.size(0)\n",
    "\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_acc = correct_test / total_test\n",
    "\n",
    "    print(f\"===== ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ =====\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cde779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ì¤‘...\n",
      "ì´ 140002ê°œ ì´ë¯¸ì§€ ê²½ë¡œ ë°œê²¬.\n",
      "20000ê°œ ìƒ˜í”Œì„ ìƒ˜í”Œë§...\n",
      "ìƒ˜í”Œë§ ì™„ë£Œ: 20000ê°œ ì´ë¯¸ì§€ ì„ íƒ.\n"
     ]
    }
   ],
   "source": [
    "# ë©”ì¸ ì‹¤í–‰\n",
    "start_time = time.time()\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ ë° ë¼ë²¨ ìˆ˜ì§‘\n",
    "Fake_PATH = \"C:/Users/user/Downloads/Dataset/Dataset/Train/Fake\"\n",
    "Real_PATH = \"C:/Users/user/Downloads/Dataset/Dataset/Train/Real\"\n",
    "MODEL_SAVE_PATH = \"./ML/deepfake_baseline_model.pth\"\n",
    "print(\"ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ ì¤‘...\")\n",
    "face_real_dir = os.path.join(Real_PATH)\n",
    "face_fake_dir = os.path.join(Fake_PATH)\n",
    "\n",
    "real_paths = glob.glob(os.path.join(face_real_dir, \"*.*\"))\n",
    "fake_paths = glob.glob(os.path.join(face_fake_dir, \"*.*\"))\n",
    "\n",
    "all_paths = real_paths + fake_paths\n",
    "all_labels = [0] * len(real_paths) + [1] * len(fake_paths)\n",
    "\n",
    "print(f\"ì´ {len(all_labels)}ê°œ ì´ë¯¸ì§€ ê²½ë¡œ ë°œê²¬.\")\n",
    "# ìƒ˜í”Œ ê°œìˆ˜ ì œí•œ\n",
    "NUM_SAMPLES = min(NUM_SAMPLES, len(all_paths))\n",
    "\n",
    "print(f\"{NUM_SAMPLES}ê°œ ìƒ˜í”Œì„ ìƒ˜í”Œë§...\")\n",
    "_, target_paths, _, target_labels = train_test_split(\n",
    "    all_paths, all_labels,\n",
    "    test_size= NUM_SAMPLES,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "print(f\"ìƒ˜í”Œë§ ì™„ë£Œ: {NUM_SAMPLES}ê°œ ì´ë¯¸ì§€ ì„ íƒ.\")\n",
    "target_paths = all_paths\n",
    "target_labels = all_labels\n",
    "    \n",
    "# ì „ì²´ë¥¼ ì“°ë”ë¼ë„ í•™ìŠµì„ ìœ„í•´ ì„ì–´ì¤ë‹ˆë‹¤.\n",
    "combined = list(zip(target_paths, target_labels))\n",
    "np.random.shuffle(combined)\n",
    "target_paths[:], target_labels[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c72b1ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ë¥¼ 7:2:1 ë¹„ìœ¨ë¡œ ë¶„í• í•©ë‹ˆë‹¤...\n",
      "ë¶„í•  ì™„ë£Œ: Train 14000ê°œ, Validation 4000ê°œ, Test 2000ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(\"ë°ì´í„°ë¥¼ 7:2:1 ë¹„ìœ¨ë¡œ ë¶„í• í•©ë‹ˆë‹¤...\")\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    target_paths, target_labels, test_size=0.3, random_state=42, stratify=target_labels\n",
    ")\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=(1/3), random_state=42, stratify=temp_labels\n",
    ")\n",
    "print(f\"ë¶„í•  ì™„ë£Œ: Train {len(train_paths)}ê°œ, Validation {len(val_paths)}ê°œ, Test {len(test_paths)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f25f561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ë¡œë“œ)...\n",
      "Loading 14000 images to RAM (uint8 mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14000/14000 [04:26<00:00, 52.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 4000 images to RAM (uint8 mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [01:18<00:00, 50.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2000 images to RAM (uint8 mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:37<00:00, 53.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ë¡œë“œ ì™„ë£Œ.\n",
      "ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ ë° RAMì— ë¡œë“œ\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ë¡œë“œ)...\")\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°ë¥¼ RAMìœ¼ë¡œ ë¡œë“œ\n",
    "X_train = load_images_to_ram_uint8(train_paths)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_val = load_images_to_ram_uint8(val_paths)\n",
    "y_val = torch.tensor(val_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test = load_images_to_ram_uint8(test_paths)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(\"ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "# RAM ê¸°ë°˜ì˜ TensorDatasetê³¼ DataLoader ìƒì„±\n",
    "train_dataset = RAMDataset(X_train, y_train, transform=None)\n",
    "val_dataset = RAMDataset(X_val, y_val, transform=None)\n",
    "test_dataset = RAMDataset(X_test, y_test, transform=None)\n",
    "\n",
    "# RAMì—ì„œ ì½ìœ¼ë¯€ë¡œ num_workers=0, pin_memory=False (ì´ë¯¸ RAMì— ìˆìŒ)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f14622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading alexnet architecture (FROM SCRATCH)...\n",
      "=== í•™ìŠµ ì‹œì‘ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - 703s - Train Loss: 0.7071, Train Acc: 0.5821 - Val Loss: 0.6171, Val Acc: 0.6783\n",
      "  Validation loss decreased (inf --> 0.6171). Saving model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./ML does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 9. í•™ìŠµ ë° í‰ê°€\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m evaluate_model(model, test_loader, criterion, device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì´ ì‹¤í–‰ ì‹œê°„: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ë¶„\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 79\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience)\u001b[0m\n\u001b[0;32m     77\u001b[0m     best_val_loss \u001b[38;5;241m=\u001b[39m epoch_val_loss\n\u001b[0;32m     78\u001b[0m     best_model_weights \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[1;32m---> 79\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ./ML does not exist."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# 8. ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "model = get_model('alexnet', device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 9. í•™ìŠµ ë° í‰ê°€\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, device, EPOCHS, PATIENCE)\n",
    "evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"ì´ ì‹¤í–‰ ì‹œê°„: {(time.time() - start_time) / 60:.2f} ë¶„\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
